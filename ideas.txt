TODO: Small utility
- Add regularization
- Add softmax activation function
- Improve precision of the metrics display during the training (Moving accuracy and SSE)
- Add a Builder named trainingConfig which can be passed to the NeuralNetwork's fit method
- Implement the HDense layer
- Avoid back and forth copies between CPU and GPU

TODO: Medium utility
- Implement the dynamic dense layer.
- Allow to save and reload the network
- Train a model on ImageNet
- Implement more complex model neural networks:
 (1) Allow non-sequential neural network
 (2) Allow network to contains networks (composite design patterns), i.e. Inception Module is a network and the GoogleNet contains many of them.
 (3) Implement ResNet, AlexNet, GoogleNet, ...

TODO: Big utility
- Retrieve results for the HCNN models and GPU implementation:
 (1) accuracy of the HCNN vs CNN
 (2) training time of the HCNN vs CNN
 (3) speed up of GPU vs CPU
 (4) Model robustness, i.e. adversarial images (section 7.13) + master thesis
 (5) Features learned, i.e. comparison with Dropout & Neuron mining
